{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark DataFrame Preprocessing for CORD-19\n",
    "\n",
    "SQL is a useful tool for querying data. [Apache Spark](https://spark.apache.org/) is a framework that allows for map-reduce workloads with a SQL-interface through the `pyspark.sql` module. The data provided by CORD-19 is semi-structured and contains many nested fields that can be tricky to work with. \n",
    "\n",
    "This notebook contains starter code for pre-processing the raw JSON documents into a structured and strongly-typed [Spark DataFrame](https://spark.apache.org/docs/latest/sql-programming-guide.html) that can be queried using Spark SQL. I'll provide a cell that can be used as the starting point for exploration into the dataset. I'll also provide a few example queries for interacting with nested data.\n",
    "\n",
    "\n",
    "### Handy References\n",
    "\n",
    "* [`spark.sql` module documentation](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html)\n",
    "* [Databricks `select` documentation on lateral view](https://docs.databricks.com/spark/latest/spark-sql/language-manual/select.html#lateral-view)\n",
    "* [Spark Data Types reference](https://spark.apache.org/docs/latest/sql-reference.html)\n",
    "\n",
    "\n",
    "### Notes on the environment\n",
    "\n",
    "To being, make sure the notebook has access to the internet. If you are running any Spark code locally, I suggest setting the `SPARK_HOME` variable so it is pointing to the local python site packages.\n",
    "\n",
    "```bash\n",
    "# in bash or zsh on MacOS or Linux\n",
    "SPARK_HOME=$(python -c \"import pyspark; print(pyspark.__path__[0])\")\n",
    "\n",
    "# in powershell on Windows\n",
    "$env:SPARK_HOME = $(python -c \"import pyspark; print(pyspark.__path__[0])\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starter Code\n",
    "\n",
    "Spark can be installed via the Python package manager, `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\r\n",
      "  Downloading pyspark-2.4.5.tar.gz (217.8 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 217.8 MB 5.4 kB/s \r\n",
      "\u001b[?25hCollecting py4j==0.10.7\r\n",
      "  Downloading py4j-0.10.7-py2.py3-none-any.whl (197 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 197 kB 43.1 MB/s \r\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\r\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-2.4.5-py2.py3-none-any.whl size=218258791 sha256=c21542a6ef4e5120293cd8753bf87edf491a6dcb5a9c49cef3a4a9c08007c75b\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/84/30/e3/c51c5cd0229631e662d29d7b578a3e5949a4c8db033ffb70aa\r\n",
      "Successfully built pyspark\r\n",
      "Installing collected packages: py4j, pyspark\r\n",
      "Successfully installed py4j-0.10.7 pyspark-2.4.5\r\n"
     ]
    }
   ],
   "source": [
    "! pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import (\n",
    "    ArrayType,\n",
    "    IntegerType,\n",
    "    MapType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "\n",
    "\n",
    "def generate_cord19_schema():\n",
    "    \"\"\"Generate a Spark schema based on the semi-textual description of CORD-19 Dataset.\n",
    "\n",
    "    This captures most of the structure from the crawled documents, and has been\n",
    "    tested with the 2020-03-13 dump provided by the CORD-19 Kaggle competition.\n",
    "    The schema is available at [1], and is also provided in a copy of the\n",
    "    challenge dataset.\n",
    "\n",
    "    One improvement that could be made to the original schema is to write it as\n",
    "    JSON schema, which could be used to validate the structure of the dumps. I\n",
    "    also noticed that the schema incorrectly nests fields that appear after the\n",
    "    `metadata` section e.g. `abstract`.\n",
    "    \n",
    "    [1] https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-03-13/json_schema.txt\n",
    "    \"\"\"\n",
    "\n",
    "    # shared by `metadata.authors` and `bib_entries.[].authors`\n",
    "    author_fields = [\n",
    "        StructField(\"first\", StringType()),\n",
    "        StructField(\"middle\", ArrayType(StringType())),\n",
    "        StructField(\"last\", StringType()),\n",
    "        StructField(\"suffix\", StringType()),\n",
    "    ]\n",
    "\n",
    "    authors_schema = ArrayType(\n",
    "        StructType(\n",
    "            author_fields\n",
    "            + [\n",
    "                # Uncomment to cast field into a JSON string. This field is not\n",
    "                # well-specified in the source.\n",
    "                StructField(\n",
    "                    \"affiliation\",\n",
    "                    StructType(\n",
    "                        [\n",
    "                            StructField(\"laboratory\", StringType()),\n",
    "                            StructField(\"institution\", StringType()),\n",
    "                            StructField(\n",
    "                                \"location\",\n",
    "                                StructType(\n",
    "                                    [\n",
    "                                        StructField(\"settlement\", StringType()),\n",
    "                                        StructField(\"country\", StringType()),\n",
    "                                    ]\n",
    "                                ),\n",
    "                            ),\n",
    "                        ]\n",
    "                    ),\n",
    "                ),\n",
    "                StructField(\"email\", StringType()),\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # used in `section_schema` for citations, references, and equations\n",
    "    spans_schema = ArrayType(\n",
    "        StructType(\n",
    "            [\n",
    "                # character indices of inline citations\n",
    "                StructField(\"start\", IntegerType()),\n",
    "                StructField(\"end\", IntegerType()),\n",
    "                StructField(\"text\", StringType()),\n",
    "                StructField(\"ref_id\", StringType()),\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # A section of the paper, which includes the abstract, body, and back matter.\n",
    "    section_schema = ArrayType(\n",
    "        StructType(\n",
    "            [\n",
    "                StructField(\"text\", StringType()),\n",
    "                StructField(\"cite_spans\", spans_schema),\n",
    "                StructField(\"ref_spans\", spans_schema),\n",
    "                # While equations don't appear in the abstract, but appear here\n",
    "                # for consistency\n",
    "                StructField(\"eq_spans\", spans_schema),\n",
    "                StructField(\"section\", StringType()),\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    bib_schema = MapType(\n",
    "        StringType(),\n",
    "        StructType(\n",
    "            [\n",
    "                StructField(\"ref_id\", StringType()),\n",
    "                StructField(\"title\", StringType()),\n",
    "                StructField(\"authors\", ArrayType(StructType(author_fields))),\n",
    "                StructField(\"year\", IntegerType()),\n",
    "                StructField(\"venue\", StringType()),\n",
    "                StructField(\"volume\", StringType()),\n",
    "                StructField(\"issn\", StringType()),\n",
    "                StructField(\"pages\", StringType()),\n",
    "                StructField(\n",
    "                    \"other_ids\",\n",
    "                    StructType([StructField(\"DOI\", ArrayType(StringType()))]),\n",
    "                ),\n",
    "            ]\n",
    "        ),\n",
    "        True,\n",
    "    )\n",
    "\n",
    "    # Can be one of table or figure captions\n",
    "    ref_schema = MapType(\n",
    "        StringType(),\n",
    "        StructType(\n",
    "            [\n",
    "                StructField(\"text\", StringType()),\n",
    "                # Likely equation spans, not included in source schema, but\n",
    "                # appears in JSON\n",
    "                StructField(\"latex\", StringType()),\n",
    "                StructField(\"type\", StringType()),\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return StructType(\n",
    "        [\n",
    "            StructField(\"paper_id\", StringType()),\n",
    "            StructField(\n",
    "                \"metadata\",\n",
    "                StructType(\n",
    "                    [\n",
    "                        StructField(\"title\", StringType()),\n",
    "                        StructField(\"authors\", authors_schema),\n",
    "                    ]\n",
    "                ),\n",
    "                True,\n",
    "            ),\n",
    "            StructField(\"abstract\", section_schema),\n",
    "            StructField(\"body_text\", section_schema),\n",
    "            StructField(\"bib_entries\", bib_schema),\n",
    "            StructField(\"ref_entries\", ref_schema),\n",
    "            StructField(\"back_matter\", section_schema),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def extract_dataframe_kaggle(spark):\n",
    "    \"\"\"Extract a structured DataFrame from the semi-structured document dump.\n",
    "\n",
    "    It should be fairly straightforward to modify this once there are new\n",
    "    documents available. The date of availability (`crawl_date`) and `source`\n",
    "    are available as metadata.\n",
    "    \"\"\"\n",
    "    base = \"/kaggle/input/CORD-19-research-challenge\"\n",
    "    crawled_date = \"2020-03-13\"\n",
    "    sources = [\n",
    "        \"noncomm_use_subset\",\n",
    "        \"comm_use_subset\",\n",
    "        \"biorxiv_medrxiv\",\n",
    "        \"pmc_custom_license\",\n",
    "    ]\n",
    "\n",
    "    dataframe = None\n",
    "    for source in sources:\n",
    "        path = f\"{base}/{crawled_date}/{source}/{source}\"\n",
    "        df = (\n",
    "            spark.read.json(path, schema=generate_cord19_schema(), multiLine=True)\n",
    "            .withColumn(\"crawled_date\", lit(crawled_date))\n",
    "            .withColumn(\"source\", lit(source))\n",
    "        )\n",
    "        if not dataframe:\n",
    "            dataframe = df\n",
    "        else:\n",
    "            dataframe = dataframe.union(df)\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Usage and Exploration\n",
    "\n",
    "Now that we've defined the helper functions, lets start to take a look at the data. First we define a new `SparkSession`, which will create or reuse an existing session. [By default](https://spark.apache.org/docs/latest/spark-standalone.html#cluster-launch-scripts), this will utilize all cores and `total_memory - 1GB` of memory.\n",
    "\n",
    "## Extracting the Data\n",
    "\n",
    "Take note of the schema, which is heavy nested and repeated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- paper_id: string (nullable = true)\n",
      " |-- metadata: struct (nullable = true)\n",
      " |    |-- title: string (nullable = true)\n",
      " |    |-- authors: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- first: string (nullable = true)\n",
      " |    |    |    |-- middle: array (nullable = true)\n",
      " |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |-- last: string (nullable = true)\n",
      " |    |    |    |-- suffix: string (nullable = true)\n",
      " |    |    |    |-- affiliation: struct (nullable = true)\n",
      " |    |    |    |    |-- laboratory: string (nullable = true)\n",
      " |    |    |    |    |-- institution: string (nullable = true)\n",
      " |    |    |    |    |-- location: struct (nullable = true)\n",
      " |    |    |    |    |    |-- settlement: string (nullable = true)\n",
      " |    |    |    |    |    |-- country: string (nullable = true)\n",
      " |    |    |    |-- email: string (nullable = true)\n",
      " |-- abstract: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- text: string (nullable = true)\n",
      " |    |    |-- cite_spans: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- start: integer (nullable = true)\n",
      " |    |    |    |    |-- end: integer (nullable = true)\n",
      " |    |    |    |    |-- text: string (nullable = true)\n",
      " |    |    |    |    |-- ref_id: string (nullable = true)\n",
      " |    |    |-- ref_spans: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- start: integer (nullable = true)\n",
      " |    |    |    |    |-- end: integer (nullable = true)\n",
      " |    |    |    |    |-- text: string (nullable = true)\n",
      " |    |    |    |    |-- ref_id: string (nullable = true)\n",
      " |    |    |-- eq_spans: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- start: integer (nullable = true)\n",
      " |    |    |    |    |-- end: integer (nullable = true)\n",
      " |    |    |    |    |-- text: string (nullable = true)\n",
      " |    |    |    |    |-- ref_id: string (nullable = true)\n",
      " |    |    |-- section: string (nullable = true)\n",
      " |-- body_text: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- text: string (nullable = true)\n",
      " |    |    |-- cite_spans: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- start: integer (nullable = true)\n",
      " |    |    |    |    |-- end: integer (nullable = true)\n",
      " |    |    |    |    |-- text: string (nullable = true)\n",
      " |    |    |    |    |-- ref_id: string (nullable = true)\n",
      " |    |    |-- ref_spans: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- start: integer (nullable = true)\n",
      " |    |    |    |    |-- end: integer (nullable = true)\n",
      " |    |    |    |    |-- text: string (nullable = true)\n",
      " |    |    |    |    |-- ref_id: string (nullable = true)\n",
      " |    |    |-- eq_spans: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- start: integer (nullable = true)\n",
      " |    |    |    |    |-- end: integer (nullable = true)\n",
      " |    |    |    |    |-- text: string (nullable = true)\n",
      " |    |    |    |    |-- ref_id: string (nullable = true)\n",
      " |    |    |-- section: string (nullable = true)\n",
      " |-- bib_entries: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: struct (valueContainsNull = true)\n",
      " |    |    |-- ref_id: string (nullable = true)\n",
      " |    |    |-- title: string (nullable = true)\n",
      " |    |    |-- authors: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- first: string (nullable = true)\n",
      " |    |    |    |    |-- middle: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |-- last: string (nullable = true)\n",
      " |    |    |    |    |-- suffix: string (nullable = true)\n",
      " |    |    |-- year: integer (nullable = true)\n",
      " |    |    |-- venue: string (nullable = true)\n",
      " |    |    |-- volume: string (nullable = true)\n",
      " |    |    |-- issn: string (nullable = true)\n",
      " |    |    |-- pages: string (nullable = true)\n",
      " |    |    |-- other_ids: struct (nullable = true)\n",
      " |    |    |    |-- DOI: array (nullable = true)\n",
      " |    |    |    |    |-- element: string (containsNull = true)\n",
      " |-- ref_entries: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: struct (valueContainsNull = true)\n",
      " |    |    |-- text: string (nullable = true)\n",
      " |    |    |-- latex: string (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |-- back_matter: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- text: string (nullable = true)\n",
      " |    |    |-- cite_spans: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- start: integer (nullable = true)\n",
      " |    |    |    |    |-- end: integer (nullable = true)\n",
      " |    |    |    |    |-- text: string (nullable = true)\n",
      " |    |    |    |    |-- ref_id: string (nullable = true)\n",
      " |    |    |-- ref_spans: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- start: integer (nullable = true)\n",
      " |    |    |    |    |-- end: integer (nullable = true)\n",
      " |    |    |    |    |-- text: string (nullable = true)\n",
      " |    |    |    |    |-- ref_id: string (nullable = true)\n",
      " |    |    |-- eq_spans: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- start: integer (nullable = true)\n",
      " |    |    |    |    |-- end: integer (nullable = true)\n",
      " |    |    |    |    |-- text: string (nullable = true)\n",
      " |    |    |    |    |-- ref_id: string (nullable = true)\n",
      " |    |    |-- section: string (nullable = true)\n",
      " |-- crawled_date: string (nullable = false)\n",
      " |-- source: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = extract_dataframe_kaggle(spark)\n",
    "df.printSchema()\n",
    "\n",
    "df.createOrReplaceTempView(\"cord19\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we register the DataFrame as a temporary table so we can run SQL. Caching can also help significantly, if there is enough memory available.\n",
    "\n",
    "## DataFrame API vs Spark SQL\n",
    "\n",
    "These APIs are interchangable, since there is a query planner that figures out the best way to accomplish the query. Having a declarative API is helpful before you dump the data in the flattened form that suits your application.\n",
    "\n",
    "I will be showing off both the Spark DataFrame interface which can be used programmatically and the SQL interface which can be adapted for use on BigQuery.\n",
    "\n",
    "#### Group By: How many papers are there in each source?\n",
    "\n",
    "One example of a source is `biorxiv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the Spark DataFrame interface...\n",
      "+------------------+------------------------+\n",
      "|            source|count(DISTINCT paper_id)|\n",
      "+------------------+------------------------+\n",
      "|   biorxiv_medrxiv|                     803|\n",
      "|   comm_use_subset|                    9000|\n",
      "|pmc_custom_license|                    1426|\n",
      "|noncomm_use_subset|                    1973|\n",
      "+------------------+------------------------+\n",
      "\n",
      "Using the Spark SQL interface...\n",
      "+------------------+------------------------+\n",
      "|            source|count(DISTINCT paper_id)|\n",
      "+------------------+------------------------+\n",
      "|   biorxiv_medrxiv|                     803|\n",
      "|   comm_use_subset|                    9000|\n",
      "|pmc_custom_license|                    1426|\n",
      "|noncomm_use_subset|                    1973|\n",
      "+------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Using the Spark DataFrame interface...\")\n",
    "df.groupBy(\"source\").agg(F.countDistinct(\"paper_id\")).show()\n",
    "\n",
    "print(\"Using the Spark SQL interface...\")\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    source,\n",
    "    COUNT(DISTINCT paper_id)\n",
    "FROM\n",
    "    cord19\n",
    "GROUP BY\n",
    "    source\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten: Who has written the most papers?\n",
    "\n",
    "Here, lets take a look at our first nested field. Each paper can have many authors. \n",
    "\n",
    "The `COLUMN.*` notation will extract all the columns from a struct into the scope of the `SELECT` clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-------+--------------------+\n",
      "|   first|middle|   last|               email|\n",
      "+--------+------+-------+--------------------+\n",
      "|  Thomas|    []|  Nolte|thomas.nolte@boeh...|\n",
      "|     Uwe|   [B]| Sleytr|uwe.sleytr@boku.a...|\n",
      "|  Youjun|    []|   Feng|   fengyj@zju.edu.cn|\n",
      "|Changjun|    []|   Wang|changjunwang@hotm...|\n",
      "| Michael|   [R]|Hamblin|hamblin@helix.mgh...|\n",
      "+--------+------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- paper_id: string (nullable = true)\n",
      " |-- first: string (nullable = true)\n",
      " |-- middle: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- last: string (nullable = true)\n",
      " |-- suffix: string (nullable = true)\n",
      " |-- affiliation: struct (nullable = true)\n",
      " |    |-- laboratory: string (nullable = true)\n",
      " |    |-- institution: string (nullable = true)\n",
      " |    |-- location: struct (nullable = true)\n",
      " |    |    |-- settlement: string (nullable = true)\n",
      " |    |    |-- country: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "authors = df.select(\"paper_id\", F.explode(\"metadata.authors\").alias(\"author\")).select(\"paper_id\", \"author.*\")\n",
    "authors.select(\"first\", \"middle\", \"last\", \"email\").where(\"email <> ''\").show(n=5)\n",
    "authors.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now count the number of distinct papers for each author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-------+--------+\n",
      "|    first|middle|   last|n_papers|\n",
      "+---------+------+-------+--------+\n",
      "|        †|    []|       |     363|\n",
      "|Christian|    []|Drosten|      73|\n",
      "|        Y|    []|       |      62|\n",
      "|    Ralph|   [S]|  Baric|      57|\n",
      "|Kwok-Yung|    []|   Yuen|      52|\n",
      "+---------+------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    authors.groupBy(\"first\", \"middle\", \"last\")\n",
    "    .agg(F.countDistinct(\"paper_id\").alias(\"n_papers\"))\n",
    "    .orderBy(F.desc(\"n_papers\"))\n",
    ").show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the [German virologist Christian Drosten](https://en.wikipedia.org/wiki/Christian_Drosten) has quite a bit to say on the matter. There also seems to be a few data quality issues, since there are quite a few papers authored by \"†\".\n",
    "\n",
    "We can also express the same query, but in Spark-flavored SQL. The `LATERAL VIEW` will be used throughout with this DataFrame for unnesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+--------+\n",
      "|    first|   last|n_papers|\n",
      "+---------+-------+--------+\n",
      "|        †|       |     367|\n",
      "|     Wang|       |      73|\n",
      "|Christian|Drosten|      73|\n",
      "|        Y|       |      69|\n",
      "|    Ralph|  Baric|      64|\n",
      "+---------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "WITH authors AS (\n",
    "    SELECT\n",
    "        paper_id,\n",
    "        author.*\n",
    "    FROM\n",
    "        cord19\n",
    "    LATERAL VIEW\n",
    "        explode(metadata.authors) AS author\n",
    ")\n",
    "SELECT\n",
    "    first,\n",
    "    last,\n",
    "    COUNT(DISTINCT paper_id) as n_papers\n",
    "FROM\n",
    "    authors\n",
    "GROUP BY\n",
    "    first,\n",
    "    last\n",
    "ORDER BY\n",
    "    n_papers DESC\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array Aggregate: Generating full abstracts\n",
    "\n",
    "One last useful trick for handling nested fields are [`array` aggregate functions](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.array). We'll take a look at `pyspark.sql.functions.array_join` for generating full abstracts.\n",
    "\n",
    "The first way involves exploding the DataFrame with the array position, and then concatenating all of the rows belonging to particular paper. This can be translated directly into SQL. The second way involves the use of User Defined Functions, which can work on data row at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+\n",
      "|            paper_id|            abstract|words|\n",
      "+--------------------+--------------------+-----+\n",
      "|0a1533470817bc5ef...|Background: Salmo...|  222|\n",
      "|0ddcfc9bedfb0a87a...|Acute viral infec...|  196|\n",
      "|1638100b254164ee9...|Background: Infla...|  206|\n",
      "|183e393843de9d6c6...|Background: Mathe...|  349|\n",
      "|1f26b5e8291ea1ddc...|An explosion of k...|  165|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# based on https://stackoverflow.com/a/50668635\n",
    "from pyspark.sql import Window\n",
    "\n",
    "abstract = (\n",
    "    df.select(\"paper_id\", F.posexplode(\"abstract\").alias(\"pos\", \"value\"))\n",
    "    .select(\"paper_id\", \"pos\", \"value.text\")\n",
    "    .withColumn(\"ordered_text\", F.collect_list(\"text\").over(Window.partitionBy(\"paper_id\").orderBy(\"pos\")))\n",
    "    .groupBy(\"paper_id\")\n",
    "    .agg(F.max(\"ordered_text\").alias(\"sentences\"))\n",
    "    .select(\"paper_id\", F.array_join(\"sentences\", \" \").alias(\"abstract\"))\n",
    "    .withColumn(\"words\", F.size(F.split(\"abstract\", \"\\s+\")))\n",
    ")\n",
    "\n",
    "abstract.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're curious how this is represented under the hood, you can take a look at the query planner. The performance is not too bad in return for ergonomics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "SortAggregate(key=[paper_id#0], functions=[max(ordered_text#329)])\n",
      "+- SortAggregate(key=[paper_id#0], functions=[partial_max(ordered_text#329)])\n",
      "   +- *(7) Project [paper_id#0, ordered_text#329]\n",
      "      +- Window [collect_list(text#324, 0, 0) windowspecdefinition(paper_id#0, pos#319 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS ordered_text#329], [paper_id#0], [pos#319 ASC NULLS FIRST]\n",
      "         +- *(6) Sort [paper_id#0 ASC NULLS FIRST, pos#319 ASC NULLS FIRST], false, 0\n",
      "            +- Exchange hashpartitioning(paper_id#0, 200)\n",
      "               +- *(5) Project [paper_id#0, pos#319, value#320.text AS text#324]\n",
      "                  +- Generate posexplode(abstract#2), [paper_id#0], false, [pos#319, value#320]\n",
      "                     +- Union\n",
      "                        :- *(1) FileScan json [paper_id#0,abstract#2] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/kaggle/input/CORD-19-research-challenge/2020-03-13/noncomm_use_subset/non..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<paper_id:string,abstract:array<struct<text:string,cite_spans:array<struct<start:int,end:in...\n",
      "                        :- *(2) FileScan json [paper_id#33,abstract#35] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/kaggle/input/CORD-19-research-challenge/2020-03-13/comm_use_subset/comm_u..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<paper_id:string,abstract:array<struct<text:string,cite_spans:array<struct<start:int,end:in...\n",
      "                        :- *(3) FileScan json [paper_id#75,abstract#77] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/kaggle/input/CORD-19-research-challenge/2020-03-13/biorxiv_medrxiv/biorxi..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<paper_id:string,abstract:array<struct<text:string,cite_spans:array<struct<start:int,end:in...\n",
      "                        +- *(4) FileScan json [paper_id#117,abstract#119] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/kaggle/input/CORD-19-research-challenge/2020-03-13/pmc_custom_license/pmc..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<paper_id:string,abstract:array<struct<text:string,cite_spans:array<struct<start:int,end:in...\n"
     ]
    }
   ],
   "source": [
    "abstract.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the SQL analogue. This may be a bit convoluted, but if you're following along, you should be ready for any sort of data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+\n",
      "|            paper_id|            abstract|words|\n",
      "+--------------------+--------------------+-----+\n",
      "|0a1533470817bc5ef...|Background: Salmo...|  222|\n",
      "|0ddcfc9bedfb0a87a...|Acute viral infec...|  196|\n",
      "|1638100b254164ee9...|Background: Infla...|  206|\n",
      "|183e393843de9d6c6...|Background: Mathe...|  349|\n",
      "|1f26b5e8291ea1ddc...|An explosion of k...|  165|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "WITH abstract AS (\n",
    "    SELECT\n",
    "        paper_id,\n",
    "        pos,\n",
    "        value.text as text\n",
    "    FROM\n",
    "        cord19\n",
    "    LATERAL VIEW\n",
    "        posexplode(abstract) AS pos, value\n",
    "),\n",
    "collected AS (\n",
    "    SELECT\n",
    "        paper_id,\n",
    "        collect_list(text) OVER (PARTITION BY paper_id ORDER BY pos) as sentences\n",
    "    FROM\n",
    "        abstract\n",
    "),\n",
    "sentences AS (\n",
    "    SELECT\n",
    "        paper_id,\n",
    "        max(sentences) as sentences\n",
    "    FROM\n",
    "        collected\n",
    "    GROUP BY\n",
    "        paper_id\n",
    ")\n",
    "SELECT\n",
    "    paper_id,\n",
    "    array_join(sentences, \" \") as abstract,\n",
    "    -- make sure the regex is being escaped properly\n",
    "    size(split(array_join(sentences, \" \"), \"\\\\\\s+\")) as words\n",
    "FROM\n",
    "    sentences\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use a User Defined Function written in Python. This is versatile, and is similar to a `pandas.Dataframe.apply`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+\n",
      "|            paper_id|            abstract|words|\n",
      "+--------------------+--------------------+-----+\n",
      "|ce6717ad3bb0da860...|E8 piuttosto che ...|  613|\n",
      "|35349bb1fc9290338...|Angiotensin conve...| 1048|\n",
      "|5667ab8a8565a702d...|The INHAND (Inter...|  199|\n",
      "|73706c27f129eddf8...|Monomolecular arr...|  149|\n",
      "|4bf7428a3d7bd1625...|Diverse pathogens...|  342|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@F.udf(\"string\")\n",
    "def join_abstract(rows) -> str:\n",
    "    return \" \".join([row.text for row in rows])\n",
    "\n",
    "(\n",
    "    df.select(\"paper_id\", join_abstract(\"abstract\").alias(\"abstract\"))\n",
    "    .where(\"abstract <> ''\")\n",
    "    # mix and match SQL using `pyspark.sql.functions.expr` or `DataFrame.selectExpr`\n",
    "    .withColumn(\"words\", F.expr(\"size(split(abstract, '\\\\\\s+'))\"))\n",
    ").show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can also be registered to use in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+\n",
      "|            paper_id|            abstract|words|\n",
      "+--------------------+--------------------+-----+\n",
      "|ce6717ad3bb0da860...|E8 piuttosto che ...|  613|\n",
      "|35349bb1fc9290338...|Angiotensin conve...| 1048|\n",
      "|0036b28fddf7e93da...|and Blautia (P = ...| 2805|\n",
      "|5db655de0fdc5e28e...|Context: Coptidis...|  238|\n",
      "|44f01ea111fa2b4bd...|The possible occu...|  193|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.udf.register(\"join_abstract\", join_abstract)\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    paper_id,\n",
    "    join_abstract(abstract) as abstract,\n",
    "    size(split(join_abstract(abstract), '\\\\\\s+')) as words\n",
    "FROM\n",
    "    cord19\n",
    "WHERE\n",
    "    size(abstract) > 1\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What next?\n",
    "\n",
    "Take this notebook, cut all of the extra cells out, and begin processing your data for text mining. Spark has an excellent [feature-extraction](https://spark.apache.org/docs/latest/ml-features) library that can be used to transform data in all sorts of ways. For example, the extracted abstracts from above can be tokenized and turned into weighted term-frequency vectors for similarity searches.\n",
    "\n",
    "Hopefully I'll be able to follow up with some interesting analysis involving recommendation systems. I'm particularly interested in the literature around the non-pharmaceudical  interventions, and I hope to curate a sensible list of the approaches that people around the world have taken to combat COVID-19. \n",
    "\n",
    "If you need help with anything related to Spark or SQL related to this notebook, feel free to reach out. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
